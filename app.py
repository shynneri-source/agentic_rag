"""
FastAPI Web Interface for Agentic RAG Agent
Provides a modern web interface for interacting with the RAG agent
"""

import asyncio
import json
from datetime import datetime
from typing import List, Dict, Any, AsyncGenerator
from fastapi import FastAPI, HTTPException
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from langchain_core.messages import HumanMessage, AIMessage

from agent.agent import graph
from agent.config import Configuration

# Kh·ªüi t·∫°o FastAPI app
app = FastAPI(
    title="Agentic RAG Agent",
    description="A modern web interface for the Agentic RAG Agent",
    version="1.0.0"
)

# Th√™m CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Models cho request/response
class ChatMessage(BaseModel):
    role: str  # "user" ho·∫∑c "assistant"
    content: str
    timestamp: str = None
    sources: List[str] = []
    stats: Dict[str, Any] = {}

class ChatRequest(BaseModel):
    message: str
    config: Dict[str, Any] = {}

class StreamMessage(BaseModel):
    type: str  # "thinking", "query_generation", "rag_search", "reflection", "answer", "complete"
    content: str
    data: Dict[str, Any] = {}
    timestamp: str

class ChatResponse(BaseModel):
    response: str
    sources: List[str] = []
    stats: Dict[str, Any] = {}
    timestamp: str

# L∆∞u tr·ªØ l·ªãch s·ª≠ chat
chat_history: List[ChatMessage] = []

@app.get("/", response_class=HTMLResponse)
async def read_root():
    """Tr·∫£ v·ªÅ trang ch·ªß HTML"""
    with open("static/index.html", "r", encoding="utf-8") as f:
        return HTMLResponse(content=f.read(), status_code=200)

@app.post("/api/chat", response_model=ChatResponse)
async def chat_with_agent(request: ChatRequest):
    """
    Endpoint ƒë·ªÉ chat v·ªõi agent
    """
    print(f"üì® Nh·∫≠n ƒë∆∞·ª£c request: {request.message}")  # Debug log
    
    try:
        # C·∫•u h√¨nh m·∫∑c ƒë·ªãnh
        default_config = {
            "query_generator_model": "qwen/qwen3-4b",
            "reflection_model": "qwen/qwen3-4b", 
            "rag_model": "qwen/qwen3-4b",
            "answer_model": "qwen/qwen3-4b",
            "max_rag_loops": 3,
            "number_of_initial_queries": 2
        }
        
        # Merge v·ªõi config t·ª´ request
        config = {
            "configurable": {**default_config, **request.config}
        }
        
        print("‚öôÔ∏è Config:", config)  # Debug log
        
        # Kh·ªüi t·∫°o state ban ƒë·∫ßu
        initial_state = {
            "user_messages": [HumanMessage(content=request.message)],
            "rag_query": [],
            "rag_query_result": [],
            "source_gathered": [],
            "initial_rag_query_count": config["configurable"]["number_of_initial_queries"],
            "max_rag_loops": config["configurable"]["max_rag_loops"],
            "rag_loop_count": 0,
            "reasoning_model": config["configurable"]["rag_model"]
        }
        
        print("üîÑ Kh·ªüi t·∫°o state th√†nh c√¥ng")  # Debug log
        
        # Th√™m message c·ªßa user v√†o l·ªãch s·ª≠
        user_message = ChatMessage(
            role="user",
            content=request.message,
            timestamp=datetime.now().isoformat()
        )
        chat_history.append(user_message)
        
        print("ü§ñ ƒêang ch·∫°y agent...")  # Debug log
        
        # Ch·∫°y agent
        result = await graph.ainvoke(initial_state, config=config)
        
        print("‚úÖ Agent ho√†n th√†nh, k·∫øt qu·∫£:", list(result.keys()))
        
        # L·∫•y response t·ª´ k·∫øt qu·∫£
        response_content = ""
        if "user_messages" in result and result["user_messages"]:
            final_message = result["user_messages"][-1]
            if hasattr(final_message, 'content'):
                response_content = final_message.content
            else:
                response_content = str(final_message)
        else:
            response_content = "Xin l·ªói, kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ agent."
        
        print(f"üì§ Response: {response_content[:100]}...")  # Debug log
        
        # T·∫°o th·ªëng k√™
        stats = {
            "rag_loop_count": result.get('rag_loop_count', 0),
            "total_queries": len(result.get('rag_query_result', [])),
            "unique_sources": len(set(result.get('source_gathered', []))),
            "processing_time": "N/A"
        }
        
        sources = list(set(result.get('source_gathered', [])))
        
        print(f"üìä Stats: {stats}")  # Debug log
        
        # T·∫°o response
        chat_response = ChatResponse(
            response=response_content,
            sources=sources,
            stats=stats,
            timestamp=datetime.now().isoformat()
        )
        
        # Th√™m response c·ªßa agent v√†o l·ªãch s·ª≠
        agent_message = ChatMessage(
            role="assistant",
            content=response_content,
            timestamp=datetime.now().isoformat(),
            sources=sources,
            stats=stats
        )
        chat_history.append(agent_message)
        
        print("‚úÖ Ho√†n th√†nh request")  # Debug log
        
        return chat_response
        
    except Exception as e:
        print(f"‚ùå L·ªói: {str(e)}")  # Debug log
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"L·ªói x·ª≠ l√Ω: {str(e)}")

@app.post("/api/chat/stream")
async def chat_with_agent_stream(request: ChatRequest):
    """
    Endpoint ƒë·ªÉ chat v·ªõi agent v·ªõi streaming
    """
    async def stream_response() -> AsyncGenerator[str, None]:
        try:
            # C·∫•u h√¨nh m·∫∑c ƒë·ªãnh
            default_config = {
                "query_generator_model": "qwen/qwen3-4b",
                "reflection_model": "qwen/qwen3-4b", 
                "rag_model": "qwen/qwen3-4b",
                "answer_model": "qwen/qwen3-4b",
                "max_rag_loops": 3,
                "number_of_initial_queries": 2
            }
            
            # Merge v·ªõi config t·ª´ request
            config = {
                "configurable": {**default_config, **request.config}
            }
            
            # Kh·ªüi t·∫°o state ban ƒë·∫ßu
            initial_state = {
                "user_messages": [HumanMessage(content=request.message)],
                "rag_query": [],
                "rag_query_result": [],
                "source_gathered": [],
                "initial_rag_query_count": config["configurable"]["number_of_initial_queries"],
                "max_rag_loops": config["configurable"]["max_rag_loops"],
                "rag_loop_count": 0,
                "reasoning_model": config["configurable"]["rag_model"]
            }
            
            # Stream thinking
            yield f"data: {json.dumps({'type': 'thinking', 'content': 'ƒêang kh·ªüi t·∫°o agent v√† ph√¢n t√≠ch c√¢u h·ªèi...', 'data': {}, 'timestamp': datetime.now().isoformat()})}\n\n"
            
            # Ch·∫°y agent v·ªõi streaming
            step_count = 0
            all_node_data = {}  # L∆∞u tr·ªØ t·∫•t c·∫£ data t·ª´ c√°c nodes
            seen_rag_queries = set()  # Theo d√µi c√°c RAG queries ƒë√£ hi·ªÉn th·ªã
            
            async for event in graph.astream(initial_state, config=config):
                step_count += 1
                print(f"üîÑ Step {step_count}: {list(event.keys())}")  # Debug log
                
                # X·ª≠ l√Ω t·ª´ng node event
                for node_name, node_data in event.items():
                    print(f"üìä Node {node_name}: {list(node_data.keys())}")  # Debug log
                    
                    # L∆∞u tr·ªØ data t·ª´ node n√†y
                    if node_name not in all_node_data:
                        all_node_data[node_name] = []
                    all_node_data[node_name].append(node_data)
                    
                    if node_name == "generate_query":
                        queries = node_data.get("rag_query", [])
                        yield f"data: {json.dumps({'type': 'query_generation', 'content': f'ƒê√£ t·∫°o {len(queries)} c√¢u truy v·∫•n ƒë·ªÉ t√¨m ki·∫øm th√¥ng tin', 'data': {'queries': queries}, 'timestamp': datetime.now().isoformat()})}\n\n"
                    
                    elif node_name == "rag_research":
                        # L·∫•y query t·ª´ state ho·∫∑c t·ª´ Send message
                        query = node_data.get("rag_query", "ƒêang t√¨m ki·∫øm...")
                        
                        # X·ª≠ l√Ω query - c√≥ th·ªÉ l√† string ho·∫∑c object t·ª´ Send message
                        query_str = "ƒêang t√¨m ki·∫øm..."
                        if isinstance(query, str):
                            query_str = query
                        elif hasattr(query, 'query'):
                            query_str = query.query
                        elif isinstance(query, dict):
                            if "query" in query:
                                query_str = query["query"]
                            elif "rag_query" in query:
                                query_str = str(query["rag_query"])
                        else:
                            query_str = str(query)
                            
                        if "rag_query_result" in node_data:
                            # T·∫°o unique key ƒë·ªÉ tr√°nh duplicate
                            query_key = f"{query_str}_{step_count}"
                            
                            # Ch·ªâ hi·ªÉn th·ªã n·∫øu ch∆∞a th·∫•y query n√†y
                            if query_key not in seen_rag_queries:
                                seen_rag_queries.add(query_key)
                                query_result = node_data.get("rag_query_result", [])
                                results_count = len(query_result) if isinstance(query_result, list) else 1
                                content_msg = f'üîé T√¨m ki·∫øm: "{query_str}" - T√¨m th·∫•y {results_count} k·∫øt qu·∫£'
                                yield f"data: {json.dumps({'type': 'rag_search', 'content': content_msg, 'data': {'query': query_str, 'results_count': results_count}, 'timestamp': datetime.now().isoformat()})}\n\n"
                    
                    elif node_name == "reflection":
                        is_sufficient = node_data.get("is_sufficient", False)
                        rag_loop_count = node_data.get("rag_loop_count", 0)
                        knowledge_gap = node_data.get("knowledge_gap", "")
                        
                        if is_sufficient:
                            content = f"‚úÖ ƒê√°nh gi√°: Th√¥ng tin ƒë√£ ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi (V√≤ng RAG: {rag_loop_count})"
                            yield f"data: {json.dumps({'type': 'reflection', 'content': content, 'data': {'is_sufficient': True, 'rag_loop_count': rag_loop_count}, 'timestamp': datetime.now().isoformat()})}\n\n"
                        else:
                            content = f"üîÑ ƒê√°nh gi√°: C·∫ßn t√¨m th√™m th√¥ng tin - {knowledge_gap} (V√≤ng RAG: {rag_loop_count})"
                            
                            # Hi·ªÉn th·ªã follow-up queries n·∫øu c√≥
                            follow_up_queries = node_data.get("follow_up_queries", [])
                            if follow_up_queries:
                                content += f" - S·∫Ω t√¨m th√™m {len(follow_up_queries)} c√¢u truy v·∫•n"
                                
                            yield f"data: {json.dumps({'type': 'reflection', 'content': content, 'data': {'is_sufficient': False, 'rag_loop_count': rag_loop_count, 'knowledge_gap': knowledge_gap, 'follow_up_queries': follow_up_queries}, 'timestamp': datetime.now().isoformat()})}\n\n"
                    
                    elif node_name == "finalize_answer":
                        if "user_messages" in node_data:
                            final_messages = node_data["user_messages"]
                            if final_messages:
                                final_message = final_messages[-1]
                                raw_content = final_message.content if hasattr(final_message, 'content') else str(final_message)
                                
                                # Clean up content: lo·∫°i b·ªè \n ƒë·∫ßu v√† cu·ªëi, trim whitespace
                                response_content = raw_content.strip()
                                
                                # L·∫•y th·ªëng k√™ t·ª´ t·∫•t c·∫£ data ƒë√£ thu th·∫≠p
                                all_rag_results = []
                                all_sources = []
                                max_rag_loop_count = 0
                                
                                # T·ªïng h·ª£p data t·ª´ t·∫•t c·∫£ nodes
                                for node_type, node_list in all_node_data.items():
                                    for data in node_list:
                                        if "rag_query_result" in data:
                                            all_rag_results.extend(data["rag_query_result"])
                                        if "source_gathered" in data:
                                            all_sources.extend(data["source_gathered"])
                                        if "rag_loop_count" in data:
                                            max_rag_loop_count = max(max_rag_loop_count, data["rag_loop_count"])
                                
                                stats = {
                                    "rag_loop_count": max_rag_loop_count,
                                    "total_queries": len(all_rag_results),
                                    "unique_sources": len(set(all_sources)),
                                    "processing_steps": step_count
                                }
                                
                                sources = list(set(all_sources))
                                
                                yield f"data: {json.dumps({'type': 'answer', 'content': response_content, 'data': {'sources': sources, 'stats': stats}, 'timestamp': datetime.now().isoformat()})}\n\n"
            
            # Stream completion v·ªõi t·ªïng h·ª£p cu·ªëi c√πng
            final_stats = {
                "total_steps": step_count,
                "processed_nodes": len(all_node_data)
            }
            yield f"data: {json.dumps({'type': 'complete', 'content': 'Ho√†n th√†nh x·ª≠ l√Ω', 'data': {'final_stats': final_stats}, 'timestamp': datetime.now().isoformat()})}\n\n"
            
        except Exception as e:
            error_msg = f"L·ªói x·ª≠ l√Ω: {str(e)}"
            yield f"data: {json.dumps({'type': 'error', 'content': error_msg, 'data': {}, 'timestamp': datetime.now().isoformat()})}\n\n"
    
    return StreamingResponse(stream_response(), media_type="text/plain; charset=utf-8")

@app.get("/api/chat/history")
async def get_chat_history():
    """L·∫•y l·ªãch s·ª≠ chat"""
    return {"messages": chat_history}

@app.delete("/api/chat/history")
async def clear_chat_history():
    """X√≥a l·ªãch s·ª≠ chat"""
    global chat_history
    chat_history = []
    return {"message": "ƒê√£ x√≥a l·ªãch s·ª≠ chat"}

@app.get("/api/health")
async def health_check():
    """Ki·ªÉm tra t√¨nh tr·∫°ng h·ªá th·ªëng"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "message": "Agentic RAG Agent ƒëang ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng"
    }

# Mount static files
app.mount("/static", StaticFiles(directory="static"), name="static")

if __name__ == "__main__":
    import uvicorn
    print("üöÄ Kh·ªüi ƒë·ªông Agentic RAG Web Interface...")
    print("üì± Truy c·∫≠p: http://localhost:3000")
    print("üìñ API Docs: http://localhost:3000/docs")
    
    uvicorn.run(
        "app:app", 
        host="0.0.0.0", 
        port=3000, 
        reload=True,
        log_level="info"
    )
